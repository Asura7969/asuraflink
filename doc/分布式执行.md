# 分布式执行
**Master, Worker, Client**
* master进程：也称之为JobManagers用于协调分布式执行。它们用来调度task，协调检查点，协调失败时恢复等。
Flink运行时至少存在一个master处理器。一个高可用的运行模式会存在多个master处理器，它们其中有一个是leader，而其他的都是standby
* worker进程：也称之为TaskManagers用于执行一个dataflow的task(或者特殊的subtask)、数据缓冲和data stream的交换。
Flink运行时至少会存在一个worker处理器

![The processes involved in executing a Flink dataflow](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/processes.svg)

 **Workers, Slots, Resources**
*  每一个worker(TaskManager)是一个JVM进程，它可能会在独立的线程上执行一个或多个subtask。为了控制一个worker能接收多少个task。worker通过task slot来进行控制（一个worker至少有一个task slot）。

*  每个task slot表示TaskManager拥有资源的一个固定大小的子集。假如一个TaskManager有三个slot，那么它会将其管理的内存分成三份给各个slot。资源slot化意味着一个subtask将不需要跟来自其他job的subtask竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的受管理的内存。

*  通过调整task slot的数量，允许用户定义subtask之间如何互相隔离。如果一个TaskManager有一个slot，那将意味着每个task group运行在独立的JVM中（该JVM可能是通过一个特定的容器启动的）。而一个TaskManager多个slot意味着更多的subtask可以共享同一个JVM。而在同一个JVM进程中的task将共享TCP连接（基于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个task的负载。

![A TaskManager with Task Slots and Tasks](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/tasks_slots.svg)

允许slot共享有两个好处：
*  Flink 集群所需的task slots数与job中最高的并行度一致。也就是说我们不需要再去计算一个程序总共会起多少个task了。
*  更容易获得更充分的资源利用。如果没有slot共享，那么非密集型操作source/flatmap就会占用同密集型操作 keyAggregation/sink 一样多的资源。如果有slot共享，将基线的2个并行度增加到6个，能充分利用slot资源，同时保证每个TaskManager能平均分配到重的subtasks。

slot共享行为可以通过API来控制，以防止不合理的共享。这个机制称之为**resource groups**，它定义了subtask可能共享的slot是什么资源。

作为一个约定俗成的规则，task slot推荐的默认值是CPU的核数。基于超线程技术，每个slot占用两个或者更多的实际线程上下文

![TaskManagers with shared Task Slots](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/slot_sharing.svg)


## Time and Windows

![Time- and Count Windows](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/windows.svg)

*   Event Time：指一个事件的创建时间。通常在event中用时间戳来描述，比如，可能是由生产事件的传感器或生产服务来附加。Flink访问事件时间戳通过时间戳分配器。
*   Ingestion time：指一个事件从source operator进入Flink dataflow的时间。
*   Processing time：每一个执行一个基于时间操作的operator的本地时间。


![Event Time, Ingestion Time, and Processing Time](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/event_ingestion_processing_time.svg)

## 状态和失败容忍

*  在dataflow中的许多操作一次只关注一个独立的事件（比如一个事件解析器），还有一些操作能记住多个独立事件的信息（比如，window operator），而这些操作被称为`stateful`（有状态的）。
*  有状态的操作，其状态被维护的地方，可以将其看作是一个内嵌的key/value存储器。状态和流一起被严格得分区和分布以供有状态的operator读取。因此，访问key/value的状态仅能在`keyed streams`中（在执行keyBy()函数之后产生keyed stream），并且只能根据当前事件的键来访问其值。对齐stream的键和状态可以确保所有的状态更新都是本地操作，在不需要事务开销的情况下保证一致性。这个对齐机制也允许Flink重新分布状态并显式调整stream的分区。


![State and Partitioning](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/state_partitioning.svg)

## Checkpoints for Fault Tolerance

Flink实现失败容忍使用了`stream replay`和`checkpoints`的混合机制。一个检查点会在流和状态中定义一个一致点，在该一致点streaming dataflow可以恢复并维持一致性（exactly-once的处理语义）。在最新的检查点之后的事件或状态更新将在input stream中被重放。

检查点的设置间隔意味着在执行时对失败容忍产生的额外开销以及恢复时间（也决定了需要被重放的事件数）。

关于检查点和容错的更多细节在 [fault tolerance docs](https://ci.apache.org/projects/flink/flink-docs-release-1.0/internals/stream_checkpointing.html).

![checkpoints and snapshots](https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/fig/checkpoints.svg)

### State Backends
给key/value构建索引的数据结构最终被存储的地方取决于状态最终存储的选择。其中一个选择是在内存中基于hash map，另一个是RocksDB。另外用来定义Hold住这些状态的数据结构，状态的最终存储也实现了基于时间点的快照机制，给key/value做快照，并将快照作为检查点的一部分来存储。

    https://ci.apache.org/projects/flink/flink-docs-release-1.0/concepts/concepts.html#top
    https://blog.csdn.net/yanghua_kobe/article/details/51298871
    http://wuchong.me/blog/2016/05/09/flink-internals-understanding-execution-resources/